With the pervasiveness of multicore architectures, multithreading is an
important---and often necessary---tool when programming for performance.
However, programming with multiple threads is generally more difficult than programming for serial execution.
Each thread has the potential to contain any bug of a serial program, and on
top of that, the uncertain interleaving of concurrent threads has the
potential for concurrency bugs (e.g., data races).

Partial taxonomies of concurrency bugs have been constructed~\cite{Farchi2003, Lu2008}, and it has been demonstrated that attacks on buggy multithreaded programs are a real concern~\cite{Yang2012}.
Much of the effort in combating this threat has gone into tools and systems which detect data races in order to aid debugging~\cite{Savage1997, Flanagan2004, Laadan2011, Pratikakis2011, Kasikci2013}.
An alternative approach is to guide multithreaded programs into memoized synchronization schedules~\cite{Cui2010}.
This approach does not dwell on race detection, but rather on removing the nondeterminism from the portions of multithreaded programs where races are most likely.
However, schedule memoization in its most automated form is still susceptible to attack whenever the attacker can trigger a different schedule by changing the input.

We address the threat of concurrency attacks from yet another angle: automated software diversity.
Software homogeneity is dangerous in that it provides economies of scale to attackers~\cite{Geer2003b}.
The relatively high cost of constructing an exploit for one bug is amortized by the opportunity to reuse that same exploit on every identical instance of the software.
As a result, automated software diversity is an active area of research for software in general~\cite{Larsen2014}.
Concurrency vulnerabilities subject a homogenous world of software to the same dangers for the same reasons.

Focusing on the applicability of diversity-based defenses to concurrency attacks, what we call ``time randomization'' is the randomization of synchronization schedules and thread interleavings, as well as the relative timing between and among threads.
The majority of diversification strategies systematized by Larsen et al.~\cite{Larsen2014}, like ASLR and instruction reordering, focus on mitigating attacks which depend on the predictability of absolute and/or relative addresses in memory (e.g., code reuse attacks or stack smashing attacks).
Time randomization instead focuses on mitigating attacks which depend on the predictability of relative thread timing.
This approach to defenses against concurrency attacks has the advantage that it does not require knowledge of the bug.
In other words, time randomization can be applied independent of bug detection and fixing.
This highlights another point---that time randomization does not exclude other defenses against concurrency attacks; they can be applied together in concert.

There are several possible ways to implement time randomization.
This work analyzes three implementations which can be considered automated software diversity transformations via functionally-invariant code insertion.
All three transformations involve the insertion of NOPs into multithreaded programs, one of which is a mostly unfocused randomization, and two of which specifically target synchronization mechanisms as a heuristic for time randomization.
We find two of the transformations tested to be effective in increasing exploit cost on two real-world concurrency bugs.
In the first bug, the most effective transformation resulted in exploit success rates of less than 0.5\% for all experiments beyond a certain threshold of NOPs inserted.
In the second bug, the most effective transformation resulted in nearly 85\% of exploit attempts taking significantly longer than the baseline exploit time (measured without time randomization).
Finally, we propose future directions including system-level time randomization for achieving the benefits of this defense with more tolerable overhead.
